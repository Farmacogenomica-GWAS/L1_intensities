{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Analysis of the Results File: L1_intensities Obtained via Tierpsy Tracker"
      ],
      "metadata": {
        "id": "sw1zVFX97KEf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Drive connection"
      ],
      "metadata": {
        "id": "WDdKl8W87Quc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LxcWYBX57QIU",
        "outputId": "968f5010-a8ef-49cc-a61f-81d4fb71c686"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Libraries"
      ],
      "metadata": {
        "id": "tEtDf7_q7dX7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import h5py\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "06wD5SVx7c2U"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inspecting file content"
      ],
      "metadata": {
        "id": "fNwdWs9e7fiq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def inspect_hdf5_datasets(file_path):\n",
        "    \"\"\"\n",
        "    Inspects an HDF5 file and prints the names and sizes of its datasets.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): The path to the HDF5 file.\n",
        "    \"\"\"\n",
        "    print(f\"\\n{'='*30}\")\n",
        "    print(f\"Inspecting file: {file_path}\")\n",
        "    print(f\"{'='*30}\")\n",
        "    try:\n",
        "        if not os.path.exists(file_path):\n",
        "            print(f\"Error: The file '{file_path}' was not found.\")\n",
        "            return  # Exit the function if the file doesn't exist\n",
        "\n",
        "        with h5py.File(file_path, 'r') as hdfid:\n",
        "            datasets_info = {}\n",
        "            for name, obj in hdfid.items():\n",
        "                if isinstance(obj, h5py.Dataset):\n",
        "                    datasets_info[name] = obj.size\n",
        "\n",
        "            if datasets_info:\n",
        "                print(\"Datasets found and their sizes:\")\n",
        "                for name, size in datasets_info.items():\n",
        "                    print(f\"  Dataset: {name}, Size: {size} elements\")\n",
        "            else:\n",
        "                print(\"No top-level datasets were found in this file.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while processing the file '{file_path}': {e}\")\n",
        "\n",
        "# Define the file path you want to analyze here.  REPLACE THIS!\n",
        "file_to_analyze = '/content/drive/MyDrive/Worms/Resultados/L1/L1_intensities/L1_intensities.hdf5'  # <--- REPLACE THIS LINE\n",
        "\n",
        "inspect_hdf5_datasets(file_to_analyze)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "76QTVYTp7iTM",
        "outputId": "7fbbd5ff-b5fd-4f0e-f481-425fe1be7916"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==============================\n",
            "Inspecting file: /content/drive/MyDrive/Worms/Resultados/L1/L1_intensities/L1_intensities.hdf5\n",
            "==============================\n",
            "Datasets found and their sizes:\n",
            "  Dataset: straighten_worm_intensity_median, Size: 2245733 elements\n",
            "  Dataset: trajectories_data_valid, Size: 17143 elements\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exporting the datasets to individuals CSV files"
      ],
      "metadata": {
        "id": "UGGcyxA07vbq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def export_hdf5_datasets_to_csv(file_path, output_dir):\n",
        "    \"\"\"\n",
        "    Exports all datasets from an HDF5 file to individual CSV files.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): The path to the HDF5 file.\n",
        "        output_dir (str): The path to the directory where the CSV files will be saved.\n",
        "    \"\"\"\n",
        "    print(f\"\\n{'='*30}\")\n",
        "    print(f\"Exporting all datasets from: {file_path}\")\n",
        "    print(f\"CSV files will be saved in: {output_dir}\")\n",
        "    print(f\"{'='*30}\")\n",
        "\n",
        "    try:\n",
        "        if not os.path.exists(file_path):\n",
        "            print(f\"Error: The file '{file_path}' was not found.\")\n",
        "            return\n",
        "\n",
        "        # Create the output directory if it doesn't exist\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "        with h5py.File(file_path, 'r') as hdf_file:\n",
        "            for dataset_name in hdf_file:\n",
        "                if isinstance(hdf_file[dataset_name], h5py.Dataset):\n",
        "                    print(f\"\\nProcessing dataset: {dataset_name}\")\n",
        "                    dataset = hdf_file[dataset_name]\n",
        "                    data = dataset[:]  # Read all data\n",
        "\n",
        "                    # Convert to Pandas DataFrame\n",
        "                    df = pd.DataFrame(data)\n",
        "\n",
        "                    # Construct the CSV file path\n",
        "                    csv_file_path = os.path.join(output_dir, f\"{dataset_name}.csv\")\n",
        "\n",
        "                    # Export to CSV\n",
        "                    df.to_csv(csv_file_path, index=False)\n",
        "                    print(f\"Dataset '{dataset_name}' successfully exported to: {csv_file_path}\")\n",
        "                else:\n",
        "                    print(f\"Skipping: '{dataset_name}' is not a dataset.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "    print(\"\\nData export process complete.\")\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Example usage:  MODIFY THESE PATHS APPROPRIATELY\n",
        "    hdf5_file_path = '/content/drive/MyDrive/Worms/Resultados/L1/L1_intensities/L1_intensities.hdf5'  # <--- Replace with your HDF5 file path\n",
        "    csv_output_directory = '/content/drive/MyDrive/Worms/Resultados/L1/L1_intensities/Datasets'  # <--- Replace with the desired output folder\n",
        "\n",
        "    export_hdf5_datasets_to_csv(hdf5_file_path, csv_output_directory)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GHYXj0Ls7v1q",
        "outputId": "cbdf56ab-9c35-4481-f409-8589903232f8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==============================\n",
            "Exporting all datasets from: /content/drive/MyDrive/Worms/Resultados/L1/L1_intensities/L1_intensities.hdf5\n",
            "CSV files will be saved in: /content/drive/MyDrive/Worms/Resultados/L1/L1_intensities/Datasets\n",
            "==============================\n",
            "Skipping: 'provenance_tracking' is not a dataset.\n",
            "\n",
            "Processing dataset: straighten_worm_intensity_median\n",
            "Dataset 'straighten_worm_intensity_median' successfully exported to: /content/drive/MyDrive/Worms/Resultados/L1/L1_intensities/Datasets/straighten_worm_intensity_median.csv\n",
            "\n",
            "Processing dataset: trajectories_data_valid\n",
            "Dataset 'trajectories_data_valid' successfully exported to: /content/drive/MyDrive/Worms/Resultados/L1/L1_intensities/Datasets/trajectories_data_valid.csv\n",
            "\n",
            "Data export process complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inspecting the provenance_tracking group"
      ],
      "metadata": {
        "id": "irOMqg19uMsg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def inspect_provenance_tracking(file_path):\n",
        "    \"\"\"\n",
        "    Inspects the 'provenance_tracking' group in an HDF5 file and prints its contents.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): The path to the HDF5 file.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with h5py.File(file_path, 'r') as hdf_file:\n",
        "            if 'provenance_tracking' in hdf_file:\n",
        "                provenance_group = hdf_file['provenance_tracking']\n",
        "                print(\"\\nInspecting 'provenance_tracking' group:\")\n",
        "\n",
        "                # Print attributes of the group\n",
        "                print(\"\\nAttributes:\")\n",
        "                for attr_name, attr_value in provenance_group.attrs.items():\n",
        "                    print(f\"  {attr_name}: {attr_value}\")\n",
        "\n",
        "                # Print items within the group (datasets or subgroups)\n",
        "                print(\"\\nItems within the group:\")\n",
        "                for item_name, obj in provenance_group.items():\n",
        "                    if isinstance(obj, h5py.Dataset):\n",
        "                        print(f\"  Dataset: {item_name}, Shape: {obj.shape}, Data Type: {obj.dtype}\")\n",
        "                    elif isinstance(obj, h5py.Group):\n",
        "                        print(f\"  Group: {item_name}\")\n",
        "                    else:\n",
        "                        print(f\"  Other: {item_name}, Type: {type(obj)}\")\n",
        "            else:\n",
        "                print(\"\\n'provenance_tracking' group not found in the HDF5 file.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Replace with the path to your HDF5 file\n",
        "    hdf5_file_path = '/content/drive/MyDrive/Worms/Resultados/L1/L1_intensities/L1_intensities.hdf5'\n",
        "    inspect_provenance_tracking(hdf5_file_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fG3chHim8PXi",
        "outputId": "eaba659a-0d3f-4312-f17e-b719af368dab"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Inspecting 'provenance_tracking' group:\n",
            "\n",
            "Attributes:\n",
            "  CLASS: b'GROUP'\n",
            "  TITLE: Empty(dtype=dtype('S1'))\n",
            "  VERSION: b'1.0'\n",
            "\n",
            "Items within the group:\n",
            "  Dataset: INT_PROFILE, Shape: (), Data Type: |S1047\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exporting the INT_PROFILE dataset"
      ],
      "metadata": {
        "id": "WXFIsqMrugyY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def export_int_profile(file_path, output_dir):\n",
        "    \"\"\"\n",
        "    Exports the 'INT_PROFILE' dataset from the 'provenance_tracking' group in an HDF5 file to a CSV file.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): The path to the HDF5 file.\n",
        "        output_dir (str): The directory where the CSV file will be saved.\n",
        "    \"\"\"\n",
        "    print(f\"\\n{'='*30}\")\n",
        "    print(f\"Exporting INT_PROFILE from: {file_path}\")\n",
        "    print(f\"CSV file will be saved in: {output_dir}\")\n",
        "    print(f\"{'='*30}\")\n",
        "\n",
        "    try:\n",
        "        if not os.path.exists(file_path):\n",
        "            print(f\"Error: The file '{file_path}' was not found.\")\n",
        "            return\n",
        "\n",
        "        # Create the output directory if it doesn't exist\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "        with h5py.File(file_path, 'r') as hdf_file:\n",
        "            if 'provenance_tracking' in hdf_file:\n",
        "                provenance_group = hdf_file['provenance_tracking']\n",
        "                if 'INT_PROFILE' in provenance_group:\n",
        "                    print(f\"\\nProcessing dataset: INT_PROFILE\")\n",
        "                    int_profile_data = provenance_group['INT_PROFILE'][()]  # Read the scalar value\n",
        "\n",
        "                    # Convert to Pandas DataFrame\n",
        "                    df = pd.DataFrame([int_profile_data])\n",
        "\n",
        "                    # Construct the CSV file path\n",
        "                    csv_file_path = os.path.join(output_dir, \"INT_PROFILE.csv\")\n",
        "\n",
        "                    # Export to CSV\n",
        "                    df.to_csv(csv_file_path, index=False)\n",
        "                    print(f\"Dataset 'INT_PROFILE' successfully exported to: {csv_file_path}\")\n",
        "                else:\n",
        "                    print(f\"Error: 'INT_PROFILE' dataset not found in 'provenance_tracking' group.\")\n",
        "            else:\n",
        "                print(f\"Error: 'provenance_tracking' group not found in the HDF5 file.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "    print(\"\\nData export process complete.\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Example usage:  MODIFY THESE PATHS APPROPRIATELY\n",
        "    hdf5_file_path = '/content/drive/MyDrive/Worms/Resultados/L1/L1_intensities/L1_intensities.hdf5'  # <--- Replace with your HDF5 file path\n",
        "    csv_output_directory = '/content/drive/MyDrive/Worms/Resultados/L1/L1_intensities/Datasets'  # <--- Replace with the desired output folder\n",
        "\n",
        "    export_int_profile(hdf5_file_path, csv_output_directory)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oi5sNroX9Sna",
        "outputId": "3d0df830-018d-4c12-b664-b44a8b2a7160"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==============================\n",
            "Exporting INT_PROFILE from: /content/drive/MyDrive/Worms/Resultados/L1/L1_intensities/L1_intensities.hdf5\n",
            "CSV file will be saved in: /content/drive/MyDrive/Worms/Resultados/L1/L1_intensities/Datasets\n",
            "==============================\n",
            "\n",
            "Processing dataset: INT_PROFILE\n",
            "Dataset 'INT_PROFILE' successfully exported to: /content/drive/MyDrive/Worms/Resultados/L1/L1_intensities/Datasets/INT_PROFILE.csv\n",
            "\n",
            "Data export process complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualization of each dataset"
      ],
      "metadata": {
        "id": "sPGSfCbGY2qq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "csv_output_directory = '/content/drive/MyDrive/Worms/Resultados/L1/L1_intensities/Datasets'\n",
        "\n",
        "print(f\"\\n{'='*30}\")\n",
        "print(f\"Visualizing the first 5 rows of the datasets in: {csv_output_directory}\")\n",
        "print(f\"{'='*30}\")\n",
        "try:\n",
        "    # Check if the output directory exists\n",
        "    if not os.path.exists(csv_output_directory):\n",
        "        print(f\"Error: The directory '{csv_output_directory}' was not found.\")\n",
        "        exit()\n",
        "\n",
        "    # Iterate through all files in the specified directory\n",
        "    for filename in os.listdir(csv_output_directory):\n",
        "        if filename.endswith(\".csv\"):  # Check if the file is a CSV file\n",
        "            file_path = os.path.join(csv_output_directory, filename)\n",
        "            try:\n",
        "                # Read the CSV file into a Pandas DataFrame\n",
        "                df = pd.read_csv(file_path)\n",
        "\n",
        "                print(f\"\\n--- File: {filename} ---\")\n",
        "                print(\"First 5 rows:\")\n",
        "                if not df.empty:\n",
        "                    print(df.head())\n",
        "                else:\n",
        "                    print(\"The DataFrame is empty.\")\n",
        "\n",
        "            except pd.errors.EmptyDataError:\n",
        "                print(f\"Warning: The file '{filename}' is empty.\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error reading the file '{filename}': {e}\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")\n",
        "\n",
        "print(\"\\nDataset visualization process completed.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oVJMP_XqZC_g",
        "outputId": "dec0b912-b473-49f2-e4ba-e3f25434e7b9"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==============================\n",
            "Visualizing the first 5 rows of the datasets in: /content/drive/MyDrive/Worms/Resultados/L1/L1_intensities/Datasets\n",
            "==============================\n",
            "\n",
            "--- File: straighten_worm_intensity_median.csv ---\n",
            "First 5 rows:\n",
            "       0       1       2      3       4       5       6       7       8  \\\n",
            "0  128.6  129.00  129.40  129.6  129.10  127.75  125.75  123.25  121.94   \n",
            "1  129.4  129.90  130.50  130.4  129.80  127.90  126.60  124.00  119.75   \n",
            "2  128.0  127.56  125.94  126.1  123.40  120.44  121.00  120.60  119.94   \n",
            "3  128.9  127.70  126.44  126.0  125.00  124.44  123.80  122.60  122.90   \n",
            "4  129.8  128.90  128.80  128.1  127.44  125.75  125.80  126.30  125.06   \n",
            "\n",
            "        9  ...     121     122     123     124     125    126     127    128  \\\n",
            "0  121.06  ...  126.56  126.44  126.94  125.94  125.56  126.8  128.80  129.9   \n",
            "1  123.50  ...  125.20  126.50  126.94  126.56  127.30  127.8  128.60  129.2   \n",
            "2  120.80  ...  126.56  128.40  129.20  130.10  130.50  130.6  130.10  130.4   \n",
            "3  122.30  ...  127.75  127.30  126.75  127.06  128.00  128.8  127.25  126.1   \n",
            "4  124.00  ...  129.10  129.90  129.80  129.40  129.40  129.5  129.90  130.0   \n",
            "\n",
            "      129    130  \n",
            "0  130.60  131.0  \n",
            "1  128.40  128.8  \n",
            "2  128.80  129.1  \n",
            "3  126.25  127.6  \n",
            "4  129.80  129.1  \n",
            "\n",
            "[5 rows x 131 columns]\n",
            "\n",
            "--- File: trajectories_data_valid.csv ---\n",
            "First 5 rows:\n",
            "   frame_number  worm_index_joined  plate_worm_id  skeleton_id    coord_x  \\\n",
            "0             0                  1              0            0  1054.9213   \n",
            "1             1                  1              1            1  1054.5070   \n",
            "2             2                  1              2            2  1054.2874   \n",
            "3             3                  1              3            3  1054.2156   \n",
            "4             6                  1              6            6  1054.3730   \n",
            "\n",
            "     coord_y  threshold  has_skeleton  roi_size   area  timestamp_raw  \\\n",
            "0  1562.7554     127.05             1      83.0  322.0            NaN   \n",
            "1  1564.2189     127.05             1      83.0  322.0            NaN   \n",
            "2  1565.1562     127.05             1      83.0  317.0            NaN   \n",
            "3  1565.6821     127.05             1      83.0  317.0            NaN   \n",
            "4  1566.1935     127.05             1      83.0  317.0            NaN   \n",
            "\n",
            "   timestamp_time  is_good_skel  skel_outliers_flag  int_map_id  \n",
            "0             NaN             1                   0           0  \n",
            "1             NaN             1                   0           1  \n",
            "2             NaN             1                   0           2  \n",
            "3             NaN             1                   0           3  \n",
            "4             NaN             1                   0           4  \n",
            "\n",
            "--- File: INT_PROFILE.csv ---\n",
            "First 5 rows:\n",
            "                                                   0\n",
            "0  b'{\"func_name\": \"getIntensityProfile\", \"func_a...\n",
            "\n",
            "Dataset visualization process completed.\n"
          ]
        }
      ]
    }
  ]
}